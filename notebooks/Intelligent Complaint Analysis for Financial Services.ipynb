{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d5fcd6",
   "metadata": {},
   "source": [
    "**Intelligent Complaint Analysis for Financial Services: Building a RAG-Powered Chatbot**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687faa4a",
   "metadata": {},
   "source": [
    "<h5> <b>Task 1: Exploratory Data Analysis and Data Preprocessing </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1b033e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\WPy64-31241\\python-3.12.4.amd64\\Lib\\site-packages\\seaborn\\_statistics.py:32: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.0)\n",
      "  from scipy.stats import gaussian_kde\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f838f1cb",
   "metadata": {},
   "source": [
    "**1) loading dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed83980",
   "metadata": {},
   "source": [
    "To load data pandas is used which is the standard library for data manipulation in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb77821",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72bcd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\you\\AppData\\Local\\Temp\\ipykernel_16964\\4193340569.py:3: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r'E:/KAIM/phase 2/Week 6/Intelligent-Complaint-Analysis-for-Financial-Services/data/complaints.csv')\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.22 GiB for an array with shape (17, 9609797) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:/KAIM/phase 2/Week 6/Intelligent-Complaint-Analysis-for-Financial-Services/data/complaints.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\python\\WPy64-31241\\python-3.12.4.amd64\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python\\WPy64-31241\\python-3.12.4.amd64\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python\\WPy64-31241\\python-3.12.4.amd64\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1966\u001b[0m         new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n\u001b[1;32m-> 1968\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mD:\\python\\WPy64-31241\\python-3.12.4.amd64\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mD:\\python\\WPy64-31241\\python-3.12.4.amd64\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python\\WPy64-31241\\python-3.12.4.amd64\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[1;32mD:\\python\\WPy64-31241\\python-3.12.4.amd64\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2139\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[0;32m   2122\u001b[0m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n\u001b[0;32m   2123\u001b[0m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2135\u001b[0m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n\u001b[0;32m   2138\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2139\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m \u001b[43m_form_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2140\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m BlockManager(blocks, axes, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2141\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\python\\WPy64-31241\\python-3.12.4.amd64\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2212\u001b[0m, in \u001b[0;36m_form_blocks\u001b[1;34m(arrays, consolidate, refs)\u001b[0m\n\u001b[0;32m   2209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype\u001b[38;5;241m.\u001b[39mtype, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m   2210\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m-> 2212\u001b[0m values, placement \u001b[38;5;241m=\u001b[39m \u001b[43m_stack_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n\u001b[0;32m   2214\u001b[0m     values \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n",
      "File \u001b[1;32mD:\\python\\WPy64-31241\\python-3.12.4.amd64\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2252\u001b[0m, in \u001b[0;36m_stack_arrays\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   2249\u001b[0m first \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   2250\u001b[0m shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(arrays),) \u001b[38;5;241m+\u001b[39m first\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m-> 2252\u001b[0m stacked \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[0;32m   2254\u001b[0m     stacked[i] \u001b[38;5;241m=\u001b[39m arr\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.22 GiB for an array with shape (17, 9609797) and data type object"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(r'E:/KAIM/phase 2/Week 6/Intelligent-Complaint-Analysis-for-Financial-Services/data/complaints.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'complaints.csv' not found. Please ensure the file is in the correct directory.\")\n",
    "    # Handle error, perhaps exit or raise a more specific exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06eef0",
   "metadata": {},
   "source": [
    "Robust error handling is crucial. The try-except block will inform the user if the dataset isn't found, preventing a hard crash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa5dbd8",
   "metadata": {},
   "source": [
    "**2) Initial EDa and data understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4eee0",
   "metadata": {},
   "source": [
    "To gain a high-level overview of the data, including column names, data types, and initial insights into data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ffcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Initial Data Overview ---\")\n",
    "print(df.info())\n",
    "print(\"\\n--- First 5 rows ---\")\n",
    "print(df.head())\n",
    "print(\"\\n--- Descriptive Statistics for Numerical Columns ---\")\n",
    "print(df.describe(include='all')) # Use include='all' for non-numerical columns too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c91b08",
   "metadata": {},
   "source": [
    "df.info() provides non-null counts and data types, immediately highlighting missing values. df.head() gives a quick glance at the data structure. df.describe(include='all') offers statistics for all columns, including unique counts for categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f63d1b1",
   "metadata": {},
   "source": [
    "**3) Analyzing Complaint Distribution Across Products**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d2acd7",
   "metadata": {},
   "source": [
    "To understand which financial products generate the most complaints. This informs the business objective and potential areas of focus for CrediTrust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Complaint Distribution Across Products\n",
    "print(\"\\n--- Distribution of Complaints by Product ---\")\n",
    "product_distribution = df['Product'].value_counts()\n",
    "print(product_distribution)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=product_distribution.index, y=product_distribution.values, palette='viridis')\n",
    "plt.title('Number of Complaints per Product')\n",
    "plt.xlabel('Product')\n",
    "plt.ylabel('Number of Complaints')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af292f",
   "metadata": {},
   "source": [
    "-value_counts() is effective for categorical distribution. Visualization using \n",
    "\n",
    "matplotlib and seaborn makes the insights easily digestible, crucial for reporting to stakeholders like Asha. The \n",
    "\n",
    "\n",
    "rotation and tight_layout ensure readability of product names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d021eeb",
   "metadata": {},
   "source": [
    "**4) Analyzing Consumer Complaint Narrative Length**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a2059",
   "metadata": {},
   "source": [
    "To identify very short or very long narratives, which might impact embedding quality and chunking strategies later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f148567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count for each narrative\n",
    "df['narrative_word_count'] = df['Consumer complaint narrative'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\n--- Narrative Length Statistics (Word Count) ---\")\n",
    "print(df['narrative_word_count'].describe())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['narrative_word_count'], bins=50, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Consumer Complaint Narrative Length (Word Count)')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Identify very short/long narratives (example thresholds)\n",
    "very_short_narratives = df[df['narrative_word_count'] < 10].shape[0]\n",
    "very_long_narratives = df[df['narrative_word_count'] > df['narrative_word_count'].quantile(0.99)].shape[0]\n",
    "print(f\"\\nNumber of very short narratives (<10 words): {very_short_narratives}\")\n",
    "print(f\"Number of very long narratives (>99th percentile): {very_long_narratives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed927dbd",
   "metadata": {},
   "source": [
    " Converting to str before split() handles potential NaN values gracefully. Histograms provide a visual understanding of the distribution, while describe() offers statistical summaries. Identifying outliers (very short/long) is important for subsequent text processing steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f141174",
   "metadata": {},
   "source": [
    "**5) identifying Complaints With/Without Narratives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5110495",
   "metadata": {},
   "source": [
    "It is essential for filtering, as the RAG system relies on these narratives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b045117",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_with_narrative = df['Consumer complaint narrative'].dropna().shape[0]\n",
    "complaints_without_narrative = df['Consumer complaint narrative'].isnull().sum()\n",
    "print(f\"\\nNumber of complaints with narratives: {complaints_with_narrative}\")\n",
    "print(f\"Number of complaints without narratives: {complaints_without_narrative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31aaa9",
   "metadata": {},
   "source": [
    "dropna() and isnull().sum() are direct methods for this check. This step directly informs the filtering requirement to remove records without narratives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbebd0d6",
   "metadata": {},
   "source": [
    "**6) Filtering the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eeb240",
   "metadata": {},
   "source": [
    "It is used to restrict the dataset to relevant products and ensure all records have a narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the five specified products\n",
    "specified_products = [\n",
    "    'Credit card',\n",
    "    'Personal loan',\n",
    "    'Buy Now, Pay Later (BNPL)',\n",
    "    'Savings account',\n",
    "    'Money transfer' # Assuming this is the correct exact name from the dataset\n",
    "]\n",
    "\n",
    "# Filter for specified products\n",
    "df_filtered_products = df[df['Product'].isin(specified_products)].copy()\n",
    "print(f\"\\nShape after filtering for specified products: {df_filtered_products.shape}\")\n",
    "\n",
    "# Remove records with empty Consumer complaint narrative fields\n",
    "df_cleaned = df_filtered_products.dropna(subset=['Consumer complaint narrative']).copy()\n",
    "print(f\"Shape after removing empty narratives: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38375d",
   "metadata": {},
   "source": [
    "Using isin() for product filtering is efficient. dropna(subset=['Consumer complaint narrative']) specifically targets the narrative column for null values. Using \n",
    "\n",
    ".copy() after filtering prevents SettingWithCopyWarning in future operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec773ef",
   "metadata": {},
   "source": [
    "**7) Cleaning Text Narratives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b5c68d",
   "metadata": {},
   "source": [
    "In order to mprove the quality of embeddings by removing noise and normalizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d024ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower() # Lowercasing [cite: 82]\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) # Remove punctuation\n",
    "    # Remove common boilerplate text (example) [cite: 83]\n",
    "    text = re.sub(r'i am writing to file a complaint', '', text)\n",
    "    text = re.sub(r'this is a complaint regarding', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "df_cleaned['Consumer complaint narrative_cleaned'] = df_cleaned['Consumer complaint narrative'].apply(clean_text)\n",
    "print(\"\\n--- Sample of cleaned narratives ---\")\n",
    "print(df_cleaned[['Consumer complaint narrative', 'Consumer complaint narrative_cleaned']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e84c4d",
   "metadata": {},
   "source": [
    "Lowercasing is standard practice. Removing punctuation helps focus embeddings on word meaning. Identifying and removing boilerplate text can significantly improve signal-to-noise ratio. Regular expressions (re) are powerful for these tasks. More advanced techniques (e.g., stemming, lemmatization, stop-word removal) could be considered based on further EDA and performance testing, as optional steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970b3f5",
   "metadata": {},
   "source": [
    "**8) Saving the Cleaned Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359a577",
   "metadata": {},
   "source": [
    "To create an intermediary artifact for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c74624",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'data/filtered_complaints.csv'\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "print(f\"\\nCleaned and filtered dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023fb9b1",
   "metadata": {},
   "source": [
    "The cleaned and filtered dataset saved to data/filtered_complaints.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579217de",
   "metadata": {},
   "source": [
    "index=False prevents Pandas from writing the DataFrame index as a column in the CSV, which is generally desired for clean data files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9d3b2",
   "metadata": {},
   "source": [
    "<h5><b>Task 2: Text Chunking, Embedding, and Vector Store Indexing</b></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b8d5cc",
   "metadata": {},
   "source": [
    "In this task we shall convert the cleaned text narratives into a format suitable for efficient semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a2ef88",
   "metadata": {},
   "source": [
    "**1) Text Chunking Strategy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c389d",
   "metadata": {},
   "source": [
    "- The long narratives can dilute the semantic meaning when embedded as a single vector. Chunking breaks them into smaller, more semantically coherent units.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c9b89",
   "metadata": {},
   "source": [
    "- The most applicable tool LangChain's RecursiveCharacterTextSplitter is highly recommended for its effectiveness in preserving semantic units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Assuming df_cleaned is loaded from 'data/filtered_complaints.csv'\n",
    "df_cleaned = pd.read_csv('data/filtered_complaints.csv')\n",
    "narratives = df_cleaned['Consumer complaint narrative_cleaned'].tolist()\n",
    "\n",
    "# Experiment with chunk_size and chunk_overlap\n",
    "# chunk_size: maximum number of characters in a chunk\n",
    "# chunk_overlap: number of characters to overlap between chunks to maintain context\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Example: 500 characters\n",
    "    chunk_overlap=100, # Example: 100 characters overlap\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Prioritized separators\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "for i, narrative in enumerate(narratives):\n",
    "    chunks = text_splitter.split_text(narrative)\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append({\n",
    "            'original_complaint_id': df_cleaned.loc[i, 'Complaint ID'], # Assuming 'Complaint ID' exists\n",
    "            'product': df_cleaned.loc[i, 'Product'],\n",
    "            'chunk_text': chunk\n",
    "        })\n",
    "\n",
    "chunks_df = pd.DataFrame(all_chunks)\n",
    "print(f\"\\nTotal chunks created: {len(chunks_df)}\")\n",
    "print(chunks_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611b3ba6",
   "metadata": {},
   "source": [
    "The choice of chunk_size and chunk_overlap is crucial. Smaller chunks might miss broader context, while larger ones might include irrelevant information. RecursiveCharacterTextSplitter attempts to split intelligently at natural breakpoints. Justification in the report would involve discussing experimentation and the rationale behind the chosen values, perhaps showing examples of chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adedfaba",
   "metadata": {},
   "source": [
    "**2) Choosing an Embedding Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbf7b1",
   "metadata": {},
   "source": [
    "- To convert text chunks into numerical vector representations.\n",
    "- In this case sentence-transformers/all-MiniLM-L6-v2 is a good balance of performance and efficiency for semantic similarity tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d274aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(embedding_model_name)\n",
    "print(f\"\\nEmbedding model '{embedding_model_name}' loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d53112",
   "metadata": {},
   "source": [
    "This model is a fine-tuned BERT model designed for semantic similarity. In the report, explain that it's chosen for its effectiveness in capturing semantic meaning in a computationally efficient manner, suitable for real-time querying. Other options exist (e.g., larger Sentence Transformers models, OpenAI embeddings), but \n",
    "\n",
    "all-MiniLM-L6-v2 is a solid starting point for its balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf3d42",
   "metadata": {},
   "source": [
    "**3) Embedding and Indexing (Vector Store Creation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc0ec0",
   "metadata": {},
   "source": [
    "- It is used to generate embeddings for each chunk and store them in a vector database for efficient similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d21df7",
   "metadata": {},
   "source": [
    "- Here FAISS (Facebook AI Similarity Search) or ChromaDB are employed choices. ChromaDB is often easier to get started with for its Python-native interface and persistence. FAISS is known for its high performance for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1250c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize ChromaDB client (persistent)\n",
    "client = chromadb.PersistentClient(path=\"./vector_store\")\n",
    "\n",
    "# Define the embedding function for ChromaDB, using the SentenceTransformer model\n",
    "# Note: ChromaDB's default embedding function might not be exactly 'all-MiniLM-L6-v2'\n",
    "# For full control and consistency, you might embed yourself and pass vectors,\n",
    "# or ensure ChromaDB is configured to use the exact model.\n",
    "# For simplicity, using a built-in one that aligns closely.\n",
    "# Alternatively, manually create embeddings and add them.\n",
    "\n",
    "# Let's manually create embeddings for better control and explicit model usage\n",
    "print(\"\\nGenerating embeddings for chunks...\")\n",
    "chunk_texts = chunks_df['chunk_text'].tolist()\n",
    "chunk_embeddings = model.encode(chunk_texts, show_progress_bar=True)\n",
    "print(\"Embeddings generated.\")\n",
    "\n",
    "# Prepare metadata for ChromaDB\n",
    "metadatas = chunks_df[['original_complaint_id', 'product']].to_dict(orient='records')\n",
    "ids = [f\"chunk_{i}\" for i in range(len(chunks_df))] # Unique IDs for each chunk\n",
    "\n",
    "collection_name = \"customer_complaints_rag\"\n",
    "try:\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name=embedding_model_name)\n",
    "    )\n",
    "    # Add chunks and their embeddings to the collection\n",
    "    collection.add(\n",
    "        embeddings=chunk_embeddings.tolist(), # Convert numpy array to list\n",
    "        documents=chunk_texts,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    print(f\"Vector store '{collection_name}' created and indexed successfully with {len(ids)} chunks.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating/adding to ChromaDB collection: {e}\")\n",
    "\n",
    "# The vector store will be persisted at './vector_store/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf31b63",
   "metadata": {},
   "source": [
    "- It's crucial to store metadata alongside each vector (e.g., original_complaint_id, product). This metadata will be vital for tracing retrieved chunks back to their source complaints and for enabling multi-product querying.\n",
    "- The get_or_create_collection method in ChromaDB is convenient for development. For FAISS, you would typically build an IndexFlatL2 or similar, add vectors, and then use faiss.write_index to save it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d030fd6",
   "metadata": {},
   "source": [
    "**Conclusion** <br>\n",
    "\n",
    "- A script (src/vector_store_creation.py or within a notebook) that performs chunking, embedding, and indexing.\n",
    "\n",
    "- The persisted vector store saved in the vector_store/ directory.\n",
    "\n",
    "- A section in the report detailing the chunking strategy (justification for chunk_size and chunk_overlap) and the embedding model choice (why all-MiniLM-L6-v2 was selected).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03d160",
   "metadata": {},
   "source": [
    "<h5><b>Task 3: Building the RAG Core Logic and Evaluation </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b767969",
   "metadata": {},
   "source": [
    "In this task we will build the retrieval and generation pipeline and, most importantly, evaluate its effectiveness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6194ad3",
   "metadata": {},
   "source": [
    "**1) Retriever Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd884fe2",
   "metadata": {},
   "source": [
    "It is used to fetch the most relevant text chunks given a user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853816bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a new script or function, load the persisted ChromaDB\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./vector_store\")\n",
    "collection_name = \"customer_complaints_rag\"\n",
    "collection = client.get_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    ")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def retrieve_chunks(question: str, k: int = 5):\n",
    "    # Embed the question\n",
    "    question_embedding = embedding_model.encode(question).tolist()\n",
    "\n",
    "    # Perform similarity search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embedding],\n",
    "        n_results=k, # top-k results [cite: 115, 116]\n",
    "        include=['documents', 'metadatas', 'distances']\n",
    "    )\n",
    "\n",
    "    retrieved_chunks_info = []\n",
    "    if results and results['documents']:\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            chunk_text = results['documents'][0][i]\n",
    "            metadata = results['metadatas'][0][i]\n",
    "            distance = results['distances'][0][i]\n",
    "            retrieved_chunks_info.append({\n",
    "                'text': chunk_text,\n",
    "                'metadata': metadata,\n",
    "                'distance': distance\n",
    "            })\n",
    "    return retrieved_chunks_info\n",
    "\n",
    "# Example usage:\n",
    "# retrieved_chunks = retrieve_chunks(\"Why are people unhappy with BNPL?\", k=5)\n",
    "# for chunk in retrieved_chunks:\n",
    "#     print(f\"Chunk: {chunk['text']}\\nMetadata: {chunk['metadata']}\\nDistance: {chunk['distance']}\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80788d7",
   "metadata": {},
   "source": [
    "- The k parameter (number of retrieved chunks) is important. Starting with k=5 is a good heuristic. The retriever should return not only the text but also the associated metadata to display sources later in the UI and for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f5e6d",
   "metadata": {},
   "source": [
    "**2) Prompt Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2663d8",
   "metadata": {},
   "source": [
    "It is used to guide the Large Language Model (LLM) to generate relevant and grounded answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a financial analyst assistant for CrediTrust. Your task is to answer questions about customer complaints.\n",
    "Use the following retrieved complaint excerpts to formulate your answer.\n",
    "If the context doesn't contain the answer, state that you don't have enough information.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11b859",
   "metadata": {},
   "source": [
    "- The provided template is excellent. It sets the persona, defines the task, emphasizes using only provided context, and instructs how to handle insufficient information. This last point (If the context doesn't contain the answer, state that you don't have enough information) is crucial for preventing hallucination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9304e9",
   "metadata": {},
   "source": [
    "**3) Generator Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f5754",
   "metadata": {},
   "source": [
    "- It is used to combine the retrieved chunks and the user's question with the prompt to generate an answer using an LLM.\n",
    "- Here tools like Hugging Face's transformers library for local LLMs, or LangChain for easy integration with various LLMs (Mistral, Llama, etc.) are employed. For a self-contained solution, a local, smaller LLM (e.g., Llama 3 8B Instruct, Mistral 7B Instruct) might be chosen if computational resources allow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86012e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# from your_rag_module import retrieve_chunks, SYSTEM_PROMPT_TEMPLATE # Assuming these are imported\n",
    "\n",
    "# Placeholder for LLM setup. In a real scenario, you'd load a specific LLM\n",
    "# For demonstration, a text generation pipeline\n",
    "# NOTE: This requires a suitable LLM to be downloaded/available, e.g., via `transformers` library\n",
    "# For actual use, consider models like 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "# or 'meta-llama/Llama-2-7b-chat-hf' if you have access and resources.\n",
    "try:\n",
    "    llm_pipeline = pipeline(\"text-generation\", model=\"distilbert/distilgpt2\", device=0) # Use device=0 for GPU if available\n",
    "    # Adjust max_new_tokens for generation length\n",
    "    # Some LLMs require specific chat templates/tokenizers\n",
    "    print(\"LLM pipeline loaded (using distilgpt2 as placeholder).\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load specified LLM. Please check model availability and resources. Error: {e}\")\n",
    "    llm_pipeline = None # Fallback if LLM can't be loaded\n",
    "\n",
    "def generate_answer(question: str):\n",
    "    if not llm_pipeline:\n",
    "        return \"Error: LLM not loaded or available.\"\n",
    "\n",
    "    retrieved_info = retrieve_chunks(question, k=5)\n",
    "    context = \"\\n\".join([item['text'] for item in retrieved_info])\n",
    "\n",
    "    if not context:\n",
    "        return \"I don't have enough information in the retrieved context to answer this question.\"\n",
    "\n",
    "    prompt = SYSTEM_PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "\n",
    "    # Generate response\n",
    "    # Note: For actual LLMs, you often need to handle generation parameters carefully\n",
    "    # like max_new_tokens, do_sample, temperature, etc.\n",
    "    # And potentially use a chat template if it's a chat-tuned model.\n",
    "    try:\n",
    "        response = llm_pipeline(prompt, max_new_tokens=250, do_sample=True, temperature=0.7, top_p=0.9)[0]['generated_text']\n",
    "        # Post-process to extract only the answer part, if the LLM repeats the prompt\n",
    "        answer_start_tag = \"Answer:\"\n",
    "        if answer_start_tag in response:\n",
    "            generated_answer = response.split(answer_start_tag, 1)[1].strip()\n",
    "        else:\n",
    "            generated_answer = response.strip() # If LLM doesn't repeat the prompt\n",
    "\n",
    "        # Return the answer and the sources for display\n",
    "        return generated_answer, [item['text'] for item in retrieved_info]\n",
    "    except Exception as e:\n",
    "        return f\"Error during LLM generation: {e}\", []\n",
    "\n",
    "# Example usage:\n",
    "# answer, sources = generate_answer(\"What are the common issues with personal loans?\")\n",
    "# print(f\"\\nAnswer: {answer}\")\n",
    "# print(f\"\\nSources: {sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74f96d",
   "metadata": {},
   "source": [
    "- The generate_answer function combines the retriever's output with the prompt and feeds it to the LLM. Careful attention to LLM generation parameters (max_new_tokens, temperature, top_p) is important for controlling output quality and creativity. \n",
    "- Post-processing the LLM's raw output to extract just the answer is often necessary as LLMs might repeat parts of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda71ebd",
   "metadata": {},
   "source": [
    "**4) Qualitative Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c2825",
   "metadata": {},
   "source": [
    "- It is crucial for understanding the system's performance and identifying areas for improvement. This is a manual, human-in-the-loop process.\n",
    "<br> <br>\n",
    "**Process:**\n",
    "- Create 5-10 representative questions (e.g., \"Why are customers complaining about credit card fees?\", \"What common fraud issues are reported with money transfers?\", \"Are there recurring problems with BNPL customer support?\").\n",
    "\n",
    "- Run each question through the generate_answer function.\n",
    "\n",
    "- Manually analyze the generated answer against the retrieved sources and the ground truth (if known).\n",
    "\n",
    "- Assign a quality score (1-5) and provide detailed comments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870aa901",
   "metadata": {},
   "source": [
    "| Question                                  | Generated Answer                                                                          | Retrieved Sources (first 1-2 relevant)        | Quality Score (1-5) | Comments/Analysis                                                                                                  |\n",
    "|-------------------------------------------|-------------------------------------------------------------------------------------------|------------------------------------------------|---------------------|--------------------------------------------------------------------------------------------------------------------|\n",
    "| Why are people unhappy with BNPL?         | Customers are frequently complaining about unexpected fees and difficulty with payment schedules based on the provided context. | \"Complaint about BNPL fees...\" \"I was charged late fee...\" | 4                   | Answer is concise and directly supported by sources. Could be slightly more detailed if more context allowed.       |\n",
    "| What are the common issues with personal loans? | I don't have enough information from the provided context to answer this question comprehensively. | (Empty or irrelevant sources)                  | 1                   | Retriever failed to find relevant chunks. Likely an issue with chunking or embedding for this specific query.       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a2ab8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a47117",
   "metadata": {},
   "source": [
    "<h5><b>Task 4: Creating an Interactive Chat Interface</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66acde1",
   "metadata": {},
   "source": [
    "Here we are going to build a user-friendly interface that allows non-technical users to interact with the RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b62297",
   "metadata": {},
   "source": [
    "**1) Choosing the Framework**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa87cf2",
   "metadata": {},
   "source": [
    "- Here Gradio or Streamlit are excellent choices for rapid prototyping and deployment of machine learning applications. Gradio is generally simpler for chat interfaces.\n",
    "- Both are easy to learn and allow for quick creation of interactive web apps without extensive web development knowledge. Gradio is often preferred for its direct chat interface components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc21ec",
   "metadata": {},
   "source": [
    "**2) Core Functionality**\n",
    "\n",
    "- Text Input Box: For users to type their questions.\n",
    "\n",
    "- Submit/Ask Button: To trigger the RAG pipeline.\n",
    "\n",
    "- Display Area: To show the AI-generated answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b6011",
   "metadata": {},
   "source": [
    "**3) Enhancing Trust and Usability (Key Requirements)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4120c766",
   "metadata": {},
   "source": [
    "- **Display Sources**: Crucial for transparency and user trust. Below the answer, display the exact text chunks that the LLM used to generate the response.\n",
    "- **Streaming**: Improve user experience by displaying the answer token-by-token, making it feel more responsive. This often requires asynchronous handling or specific LLM library features.\n",
    "- **Clear Button**: To reset the chat conversation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911f900",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
